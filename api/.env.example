# Citation Intent Classification API Environment Variables
# Copy this file to .env and customize as needed

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Inference API Configuration (OpenAI-compatible)
# Choose one of the following based on your inference backend:

# For vLLM:
INFERENCE_API_BASE_URL=http://localhost:8080/v1
INFERENCE_API_KEY=

# For TGI (Text Generation Inference):
# INFERENCE_API_BASE_URL=http://localhost:8080/v1
# INFERENCE_API_KEY=

# For LM Studio:
# INFERENCE_API_BASE_URL=http://localhost:1234/v1
# INFERENCE_API_KEY=lm-studio

# For OpenAI:
# INFERENCE_API_BASE_URL=https://api.openai.com/v1
# INFERENCE_API_KEY=your-openai-api-key

# Model Configuration
MODEL_NAME=Qwen/Qwen2.5-14B-Instruct

# Logging
LOG_LEVEL=INFO
